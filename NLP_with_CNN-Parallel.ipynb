{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Routine to Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import traceback\n",
    "import re, string\n",
    "def clean_str(string):\n",
    "  \"\"\"\n",
    "  String cleaning before vectorization\n",
    "  \"\"\"\n",
    "  try:\n",
    "    string = re.sub(r'^https?:\\/\\/<>.*[\\r\\n]*', '', string, flags=re.MULTILINE)\n",
    "    string = re.sub(r\"[^A-Za-z]\", \" \", string)         \n",
    "    words = string.strip().lower().split()    \n",
    "    words = [w for w in words if len(w)>1]\n",
    "    #return words list\n",
    "    return words\n",
    "  except:\n",
    "    print (traceback.print_exc())\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, Clean and Split Data into Training and Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load training and Test data\n",
    "import pandas as pd\n",
    "df = pd.read_csv('labeledTrainData.tsv', header=0, delimiter=\"\\t\", quoting=3)\n",
    "df['cleanReview'] = df['review'].apply(clean_str)\n",
    "\n",
    "\n",
    "#Splitting data in test and training\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train_df.reset_index(inplace=True)\n",
    "test_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Gensim Word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anand\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n",
      "2017-08-29 10:48:20,304 : INFO : loading Word2Vec object from /tmp/w2v-movie-review\n",
      "2017-08-29 10:48:21,093 : INFO : loading wv recursively from /tmp/w2v-movie-review.wv.* with mmap=None\n",
      "2017-08-29 10:48:21,093 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-08-29 10:48:21,093 : INFO : setting ignored attribute cum_table to None\n",
      "2017-08-29 10:48:21,093 : INFO : loaded /tmp/w2v-movie-review\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(28296, 300)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "model = gensim.models.Word2Vec.load('/tmp/w2v-movie-review')\n",
    "\n",
    "#Display model shape\n",
    "embedding_size = model.wv.syn0.shape[1] #How many features per word\n",
    "model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document to Matrix Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def doc2Matrix(df,start, end, n_words):\n",
    "    batch = np.empty(shape=(end-start,n_words,embedding_size))\n",
    "    for i in range(start,end):\n",
    "        words_in_doc = df['cleanReview'][i-start]\n",
    "        for j in range(n_words):\n",
    "            if (j < len(words_in_doc)):\n",
    "                try:\n",
    "                    batch[i,j] = model.wv[words_in_doc[j]]\n",
    "                except:\n",
    "                    batch[i,j] = np.random.uniform(-0.25,0.25,embedding_size)  #Unknown word\n",
    "            else:\n",
    "                batch[i,j] = np.random.uniform(-0.25,0.25,embedding_size)   #Padding\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 200, 300)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = doc2Matrix(train_df,0,100,200)\n",
    "xx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logs_path='/tmp/session9/cnn-nlp-parallel/v1'  #Place to store Model and events\n",
    "save_path=logs_path + '/'   \n",
    "max_learning_rate = 0.003 #0.03, 0.0001, 0.0002\n",
    "min_learning_rate = 0.0001\n",
    "n_classes = 2\n",
    "batch_size = 100\n",
    "n_words = 200 #How many words per document\n",
    "training_epochs = 2\n",
    "train_pkeep = 0.75\n",
    "test_pkeep = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build TensorFlow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# three convolutional layers with their channel counts, and a\n",
    "# fully connected layer (tha last layer has 10 softmax neurons)\n",
    "K = 24  # first convolutional layer output depth\n",
    "L = 24  # second convolutional layer output depth\n",
    "M = 24  # third convolutional layer\n",
    "N = 200  # fully connected layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#Define Inputs\n",
    "with tf.name_scope('input'):\n",
    "    # None -> batch size can be any size, with n_features\n",
    "    x = tf.placeholder(tf.float32, shape=[None, n_words, embedding_size], name=\"x-input\") \n",
    "    # target n_classes output classes\n",
    "    y_ = tf.placeholder(tf.int32, shape=[None], name=\"y-input\")\n",
    "    \n",
    "    #dropout rate\n",
    "    pkeep = tf.placeholder(tf.float32)\n",
    "    X1 = tf.reshape(x,[-1,n_words, embedding_size,1])    #reshape format for input to 2 dimension\n",
    "    y_one_hot = tf.one_hot(indices=y_,depth=2)  #One hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Conv1\"):    \n",
    "    W1 = tf.Variable(tf.truncated_normal([3, embedding_size, 1, K], stddev=0.1))  # 5x5 patch, 1 input channel, K output channels\n",
    "    b1 = tf.Variable(tf.ones([K])/10)\n",
    "    stride = 1\n",
    "    Y1C = tf.nn.conv2d(X1, W1, strides=[1, stride, stride, 1], padding='VALID')\n",
    "    Y1 = tf.nn.relu(Y1C + b1)\n",
    "    pool1 = tf.nn.max_pool(Y1,\n",
    "                          ksize=[1,n_words-3+1,1,1],\n",
    "                          strides=[1,1,1,1],\n",
    "                          padding='VALID',\n",
    "                          name='pool1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Conv1/pool1:0' shape=(?, 1, 1, 24) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool1  #Display dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Conv2\"):\n",
    "    W2 = tf.Variable(tf.truncated_normal([4, embedding_size, 1, L], stddev=0.1))  # 5x5 patch, K input channel, L output channels\n",
    "    b2 = tf.Variable(tf.ones([L])/10)\n",
    "    stride = 1\n",
    "    #Y2C = tf.nn.conv2d(Y1, W2, strides=[1, stride, stride, 1], padding='SAME')\n",
    "    Y2C = tf.nn.conv2d(X1, W2, strides=[1, stride, stride, 1], padding='VALID')\n",
    "    Y2 = tf.nn.relu(Y2C + b2)\n",
    "    \n",
    "    pool2 = tf.nn.max_pool(Y2,\n",
    "                          ksize=[1,n_words-4+1,1,1],\n",
    "                          strides=[1,1,1,1],\n",
    "                          padding='VALID',\n",
    "                          name='pool2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Conv2/pool2:0' shape=(?, 1, 1, 24) dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2  #Display dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Conv3\"):\n",
    "    W3 = tf.Variable(tf.truncated_normal([5, embedding_size, 1, M], stddev=0.1))  # 4x4 patch, L input channel, M output channels\n",
    "    b3 = tf.Variable(tf.ones([M])/10)\n",
    "    stride = 1\n",
    "    Y3C = tf.nn.conv2d(X1, W3, strides=[1, stride, stride, 1], padding='VALID')\n",
    "    Y3 = tf.nn.relu(Y3C + b3)\n",
    "    pool3 = tf.nn.max_pool(Y3,\n",
    "                          ksize=[1,n_words-5+1,1,1],\n",
    "                          strides=[1,1,1,1],\n",
    "                          padding='VALID',\n",
    "                          name='pool3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Conv3/pool3:0' shape=(?, 1, 1, 24) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool3 #display information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Output\"):\n",
    "    pool = tf.concat([pool1,pool2,pool3],1)  \n",
    "    YY = tf.reshape(pool, shape=[-1, K+L+M])\n",
    "    Y4de = tf.nn.dropout(YY,pkeep)\n",
    "    # y is our prediction\n",
    "    W = tf.Variable(tf.truncated_normal([K+L+M, n_classes] ,stddev=0.1))\n",
    "    b = tf.Variable(tf.zeros([n_classes]))\n",
    "    Ylogits = tf.matmul(Y4de, W) + b\n",
    "    y = tf.nn.softmax(Ylogits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Loss, Optimization, Accuracy and Logging Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify cost function\n",
    "with tf.name_scope('Loss'):    \n",
    "    #cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=y_)\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=y_one_hot)\n",
    "    cross_entropy = tf.reduce_mean(cross_entropy)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify optimizer\n",
    "with tf.name_scope('train'):\n",
    "    # optimizer is an \"operation\" which we can execute in a session\n",
    "    learn_rate = tf.placeholder(tf.float32)\n",
    "    #train_op = tf.train.GradientDescentOptimizer(learn_rate).minimize(cross_entropy)\n",
    "    train_op = tf.train.AdamOptimizer(learn_rate).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('Accuracy'):\n",
    "    # Prediction\n",
    "    prediction = tf.argmax(y,1,name=\"Predict\")\n",
    "    #Accuracy\n",
    "    #correct_prediction = tf.equal(prediction, tf.argmax(y_,1))\n",
    "    correct_prediction = tf.equal(prediction, tf.argmax(y_one_hot,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32),name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a summary for our cost and accuracy\n",
    "training_learn_rate = tf.summary.scalar(\"learning_rate\", learn_rate)\n",
    "training_loss = tf.summary.scalar(\"training_loss\", cross_entropy)\n",
    "training_accuracy = tf.summary.scalar(\"training_accuracy\", accuracy)\n",
    "test_loss = tf.summary.scalar(\"test_loss\", cross_entropy)\n",
    "test_accuracy = tf.summary.scalar(\"test_accuracy\", accuracy)\n",
    "#Create a Saver to save the graph\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executing Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Test Accuracy 0.838\n",
      "Accuracy:  0.8548\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "#Start Graph execution\n",
    "testX = doc2Matrix(test_df, 0, test_df.shape[0], n_words)\n",
    "testY = test_df['sentiment']\n",
    "with tf.Session() as sess:\n",
    "    # variables need to be initialized before we can use them\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # create log writer object\n",
    "    writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "    # perform training cycles\n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        # number of batches in one epoch\n",
    "        #batch_count = int(trainX.shape[0]/batch_size)\n",
    "        batch_count = int(train_df.shape[0]/batch_size)\n",
    "        total_decay_steps = training_epochs * batch_count\n",
    "        \n",
    "        for i in range(batch_count):            \n",
    "            batch_df = train_df[i*batch_size:i*batch_size+batch_size]\n",
    "            batch_df.reset_index(inplace=True)\n",
    "            batch_x = doc2Matrix(batch_df,0,batch_size,n_words)\n",
    "            batch_y = batch_df['sentiment']\n",
    "            \n",
    "            current_step =  epoch * batch_count + i\n",
    "            lr = min_learning_rate +(max_learning_rate-min_learning_rate)*math.exp(-current_step/total_decay_steps)\n",
    "\n",
    "            # perform the operations we defined earlier on batch\n",
    "            _,acc,loss,lr_value = sess.run([train_op, training_accuracy,training_loss,training_learn_rate], feed_dict={x: batch_x, y_: batch_y, learn_rate: lr, pkeep: train_pkeep})\n",
    "         \n",
    "            #log training accuracy and loss\n",
    "            writer.add_summary(acc, epoch * batch_count + i)\n",
    "            writer.add_summary(loss, epoch * batch_count + i)   \n",
    "            writer.add_summary(lr_value, epoch * batch_count + i)            \n",
    "                        \n",
    "        #Test loss and accuracy\n",
    "        test_acc,acc,loss,a_loss = sess.run([accuracy,test_accuracy,test_loss,cross_entropy],feed_dict={x: testX, y_: testY, pkeep: test_pkeep})\n",
    "        writer.add_summary(acc, epoch * batch_count + i)\n",
    "        writer.add_summary(loss, epoch * batch_count + i)   \n",
    "        if epoch % 5 == 0: \n",
    "            print (\"Epoch: \", epoch)\n",
    "            print ('Test Accuracy',test_acc)\n",
    "                \n",
    "    print (\"Accuracy: \", accuracy.eval(feed_dict={x: testX, y_: testY, pkeep: test_pkeep}))\n",
    "    \n",
    "    #Save the model\n",
    "    saver.save(sess, save_path + \"model.ckpt\")\n",
    "    print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
